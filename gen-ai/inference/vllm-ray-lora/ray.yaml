---
# Source: vllm-ray/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-vllm-ray-hf-token
  namespace: default
data:
  hf-token: $HF_TOKEN
---
# Source: vllm-ray/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-vllm-ray-serve
  namespace: default
  labels:
    app.kubernetes.io/name: vllm-ray
data:
  serve.py: |-
    # __serve_example_begin__
    from typing import Dict, Optional, List
    import logging
    
    from fastapi import FastAPI
    from starlette.requests import Request
    from starlette.responses import StreamingResponse, JSONResponse
    
    from ray import serve
    
    from huggingface_hub import snapshot_download
    
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.engine.metrics import RayPrometheusStatLogger
    
    from vllm.entrypoints.openai.cli_args import make_arg_parser
    from vllm.entrypoints.openai.protocol import (
        ChatCompletionRequest,
        ChatCompletionResponse,
        ErrorResponse,
    )
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
    from vllm.entrypoints.openai.serving_models import (
        BaseModelPath,
        LoRAModulePath,
        PromptAdapterPath,
        OpenAIServingModels,
    )
    
    from vllm.usage.usage_lib import UsageContext
    from vllm.utils import FlexibleArgumentParser
    from vllm.entrypoints.logger import RequestLogger
    
    logger = logging.getLogger("ray.serve")
    
    app = FastAPI()
    
    def get_served_model_names(engine_args: AsyncEngineArgs) -> List[str]:
        if engine_args.served_model_name is not None:
            served_model_names: Union[str, List[str]] = engine_args.served_model_name
            # Because the typing suggests it could be a string or list of strings
            if isinstance(served_model_names, str):
                served_model_names: List[str] = [served_model_names]
        else:
            served_model_names: List[str] = [engine_args.model]
        return served_model_names
    
    @serve.deployment(
        autoscaling_config={
            "min_replicas": 1,
            "max_replicas": 10,
            "target_ongoing_requests": 5,
        },
        max_ongoing_requests=10,
    )
    @serve.ingress(app)
    class VLLMDeployment:
        def __init__(
            self,
            engine_args: AsyncEngineArgs,
            response_role: str,
            lora_modules: Optional[List[LoRAModulePath]] = None,
            prompt_adapters: Optional[List[PromptAdapterPath]] = None,
            request_logger: Optional[RequestLogger] = None,
            chat_template: Optional[str] = None,
        ):
            logger.info(f"Starting with engine args: {engine_args}")
            self.openai_serving_chat = None
            self.engine_args = engine_args
            self.response_role = response_role
            self.lora_modules = lora_modules
            self.prompt_adapters = prompt_adapters
            self.request_logger = request_logger
            self.chat_template = chat_template
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,
            self.engine_config = self.engine_args.create_engine_config(self.usage_context)
            served_model_names: List[str] = get_served_model_names(engine_args)
            additional_metrics_logger: RayPrometheusStatLogger = RayPrometheusStatLogger(
                local_interval=0.5,
                labels=dict(model_name=served_model_names[0]),
                vllm_config=self.engine_config
            )
            self.engine.add_logger("ray", additional_metrics_logger)
    
    
        @app.post("/v1/chat/completions")
        async def create_chat_completion(
            self, request: ChatCompletionRequest, raw_request: Request
        ):
            """OpenAI-compatible HTTP endpoint.
    
            API reference:
                - https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
            """
            if not self.openai_serving_chat:
                model_config = await self.engine.get_model_config()
                # Determine the name of the served model for the OpenAI client.
                models = OpenAIServingModels(
                    self.engine,
                    model_config,
                    [
                        BaseModelPath(
                            name=self.engine_args.model, model_path=self.engine_args.model
                        )
                    ],
                    lora_modules=self.lora_modules,
                    prompt_adapters=self.prompt_adapters,
                )
                logger.info(f"static loras: {models.static_lora_modules}")
                await models.init_static_loras()
                self.openai_serving_chat = OpenAIServingChat(
                    self.engine,
                    model_config,
                    models,
                    self.response_role,
                    request_logger=self.request_logger,
                    chat_template=self.chat_template,
                    chat_template_content_format="auto",
                )
            logger.info(f"self.lora_modules: {self.lora_modules}")
            logger.info(f"Request: {request}")
            generator = await self.openai_serving_chat.create_chat_completion(
                request, raw_request
            )
            if isinstance(generator, ErrorResponse):
                return JSONResponse(
                    content=generator.model_dump(), status_code=generator.code
                )
            if request.stream:
                return StreamingResponse(content=generator, media_type="text/event-stream")
            else:
                assert isinstance(generator, ChatCompletionResponse)
                return JSONResponse(content=generator.model_dump())
    
    
    def parse_vllm_args(cli_args: Dict[str, str]):
        """Parses vLLM args based on CLI inputs.
    
        Currently uses argparse because vLLM doesn't expose Python models for all of the
        config options we want to support.
        """
        arg_parser = FlexibleArgumentParser(
            description="vLLM OpenAI-Compatible RESTful API server."
        )
    
        parser = make_arg_parser(arg_parser)
        arg_strings = []
        for key, value in cli_args.items():
            if value is None: # for cases when flag without value, yaml empty or null/None eg: --enable-lora
                arg_strings.extend([f"--{key}"])
            elif key == "lora-modules":
                arg_strings.extend([f"--{key}", *value.split(" ")])
            else:
                arg_strings.extend([f"--{key}", f"{value}"])
        logger.info(arg_strings)
        parsed_args = parser.parse_args(args=arg_strings)
        return parsed_args
    
    
    def build_app(cli_args: Dict[str, str]) -> serve.Application:
        """Builds the Serve app based on CLI arguments.
    
        See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server
        for the complete set of arguments.
    
        Supported engine arguments: https://docs.vllm.ai/en/latest/models/engine_args.html.
        """  # noqa: E501
        if "accelerator" in cli_args.keys():
            accelerator = cli_args.pop("accelerator")
        else:
            accelerator = "GPU"
    
        parsed_args = parse_vllm_args(cli_args)
        engine_args = AsyncEngineArgs.from_cli_args(parsed_args)
        engine_args.worker_use_ray = True
    
        tp = engine_args.tensor_parallel_size
        logger.info(f"Tensor parallelism = {tp}")
        pg_resources = []
        pg_resources.append({"CPU": 1})  # for the deployment replica
        for i in range(tp):
            pg_resources.append({"CPU": 1, accelerator: 1})  # for the vLLM actors
    
        # We use the "STRICT_PACK" strategy below to ensure all vLLM actors are placed on
        # the same Ray node.
        #    placement_group_bundles=pg_resources, placement_group_strategy="STRICT_PACK"
        logger.info(f"Lora modules: {parsed_args.lora_modules}")
        return VLLMDeployment.bind(
            engine_args,
            parsed_args.response_role,
            parsed_args.lora_modules,
            parsed_args.prompt_adapters,
            cli_args.get("request_logger"),
            parsed_args.chat_template,
        )
    
    
    # __serve_example_end__
    
    if __name__ == "__main__":
        serve.run(
            build_app(
                {
                    "model": "NousResearch/Meta-Llama-3-8B-Instruct",
                    "tensor-parallel-size": "1",
                }
                       )
        )
        # __query_example_begin__
        from openai import OpenAI
    
        # Note: Ray Serve doesn't support all OpenAI client arguments and may ignore some.
        client = OpenAI(
            # Replace the URL if deploying your app remotely
            # (e.g., on Anyscale or KubeRay).
            base_url="http://localhost:8000/v1",
            api_key="NOT A REAL KEY",
        )
        chat_completion = client.chat.completions.create(
            model="NousResearch/Meta-Llama-3-8B-Instruct",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {
                    "role": "user",
                    "content": "What are some highly rated restaurants in San Francisco?'",
                },
            ],
            temperature=0.01,
            stream=True,
            max_tokens=100,
        )
    
        for chat in chat_completion:
            if chat.choices[0].delta.content is not None:
                print(chat.choices[0].delta.content, end="")
        # __query_example_end__
---
# Source: vllm-ray/templates/rayservice.yaml
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: release-name-vllm-ray
  namespace: default
spec:
  serviceUnhealthySecondThreshold: 1800
  deploymentUnhealthySecondThreshold: 1800
  serveConfigV2: |
    applications:
      - name: llama-lora
        import_path: serve:build_app
        route_prefix: "/"
        args:
          chat-template: "/data/chat-templates/tool_chat_template_llama3.1_json.jinja"
          model: "meta-llama/Meta-Llama-3-8B"
          max-model-len: "8192"
          max-num-batched-tokens: "16384"
          trust-remote-code:
          tensor-parallel-size: "1"
          enable-lora:
          lora-modules: "oasst=/data/models/kaitchup/Meta-Llama-3-8B-oasst-Adapter/snapshots/c3a196d8020a286e3563ef6c2e3012286969e96d xlam=/data/models/kaitchup/Meta-Llama-3-8B-xLAM-Adapter/snapshots/e2e336c48d57ea29a3b70a533cca2ea961496d6c"
        runtime_env:
          env_vars:
            LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
            VLLM_ALLOW_RUNTIME_LORA_UPDATING: "True"
        deployments:
          - name: VLLMDeployment
            autoscaling_config:
              min_replicas: 1
              max_replicas: 2
              target_num_ongoing_requests_per_replica: 32
            ray_actor_options:
              num_cpus: 8
              num_gpus: 1
  rayClusterConfig:
    rayVersion: "2.41.0"
    enableInTreeAutoscaling: true
    headGroupSpec:
      headService:
        type: ClusterIP
      serviceType: ClusterIP
      rayStartParams:
        dashboard-host: "0.0.0.0"
        block: "true"
      template:
        spec:
          containers:
          - name: ray-head
            image: public.ecr.aws/data-on-eks/ray-2.41.0-py310-cu118-vllm0.7.0:latest
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: 4
                memory: 16Gi
              requests:
                cpu: 2
                memory: 8Gi
            env:
              - name: VLLM_PORT
                value: "8004"
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: release-name-vllm-ray-hf-token
                    key: hf-token
              - name: RAY_GRAFANA_HOST
                value: http://kube-prometheus-stack-grafana.kube-prometheus-stack.svc:80
              - name: RAY_PROMETHEUS_HOST
                value: http://kube-prometheus-stack-prometheus.kube-prometheus-stack.svc:9090
            ports:
              - containerPort: 6379
                name: gcs
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8000
                name: serve
            volumeMounts:
              - mountPath: /tmp/ray
                name: ray-logs
              - name: vllm-script
                mountPath: /home/ray/serve.py
                subPath: serve.py
              - mountPath: /data
                name: persistent-storage
          volumes:
            - name: ray-logs
              emptyDir: {}
            - name: vllm-script
              configMap:
                name: release-name-vllm-ray-serve
            - name: persistent-storage
              persistentVolumeClaim:
                claimName: efs-claim
          nodeSelector:
            NodeGroupType: x86-cpu-karpenter
            type: karpenter
    workerGroupSpecs:
      - groupName: gpu-worker-group
        replicas: 1
        minReplicas: 1
        maxReplicas: 2
        rayStartParams:
          block: "true"
        template:
          spec:
            containers:
              - name: ray-worker
                image: public.ecr.aws/data-on-eks/ray-2.41.0-py310-cu118-vllm0.7.0:latest
                imagePullPolicy: IfNotPresent
                env:
                  - name: VLLM_PORT
                    value: "8004"
                  - name: HUGGING_FACE_HUB_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: release-name-vllm-ray-hf-token
                        key: hf-token
                  - name: RAY_GRAFANA_HOST
                    value: http://kube-prometheus-stack-grafana.kube-prometheus-stack.svc:80
                  - name: RAY_PROMETHEUS_HOST
                    value: http://kube-prometheus-stack-prometheus.kube-prometheus-stack.svc:9090
                resources:
                  limits:
                    cpu: 32
                    memory: 64Gi
                    nvidia.com/gpu: 1
                  requests:
                    cpu: 16
                    memory: 32Gi
                    nvidia.com/gpu: 1
                volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
                  - name: vllm-script
                    mountPath: /home/ray/serve.py
                    subPath: serve.py
                  - mountPath: /data
                    name: persistent-storage
            volumes:
              - name: dshm
                emptyDir:
                  medium: Memory
              - name: vllm-script
                configMap:
                  name: release-name-vllm-ray-serve
              - name: persistent-storage
                persistentVolumeClaim:
                  claimName: efs-claim
            nodeSelector:
              NodeGroupType: g5-gpu-karpenter
              type: karpenter
